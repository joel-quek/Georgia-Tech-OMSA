{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Web APIs and NLP\n",
    "\n",
    "_Authors: Joel Quek (SG)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Working Notebook**\n",
    "    \n",
    "        a. Background\n",
    "\n",
    "        b. Executive Summary\n",
    "\n",
    "        c. Problem Statement\n",
    "\n",
    "        d. Exploratory Data Analysis\n",
    "\n",
    "        e. Model Evaluation\n",
    "\n",
    "        f. ROCAUC Scores\n",
    "\n",
    "        g. Conclusions and Recommendations\n",
    "\n",
    "\n",
    "\n",
    "2. **Additional Notebooks**\n",
    "\n",
    "        a. Reddit Scrape\n",
    "\n",
    "        b. Random Forest Model\n",
    "\n",
    "        c. Naive Bayes Model\n",
    "\n",
    "        d. Logistic Regression Model\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background - The Rise of Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stock selection has evolved over the years - from the study of balance sheets Fundamental Analysis to the analysis of chart patterns in Technical Analysis. These two methodologies have been the bread and butter of hedge funds before the emergence of social media. However, with the emergence of content aggregation platforms like Twitter and Reddit, the Market was at the mercy of what is now known as ['Reddit Stocks'](https://finance.yahoo.com/news/12-best-reddit-stocks-invest-233132376.html#:~:text=Leading%20subreddits%20like%20r%2FWallStreetBets,respectively%2C%20as%20of%20Q3%202022.).\n",
    "\n",
    "As an analyst in a hedge fund, I acknowledge the effect of public sentiment on price action, and how the general population is in fact a worthy opponent to the financial institutions when it comes to making market waves. Therefore, we now define a third methodology of stock selection - Sentiment Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the modern era where millenials dominate the thought-space, I propose to leverage on Sentiment Analysis as a worthy complement to Fundamental and Technical Analysis.\n",
    "\n",
    "As an analyst in a hedge fund, I would like to leverage on the [top investing and trading communities](https://www.investopedia.com/reddit-top-investing-and-trading-communities-5189322) on Reddit. The subreddit, r/wallstreetbets, is a clear favourite with more than 10 million members. However, the next in line would be two popular Subreddits\n",
    "\n",
    "1. [r/StockMarket](https://subredditstats.com/r/StockMarket) with 2,493,511\tmembers\n",
    "2. [r/investing](https://subredditstats.com/r/investing) with 2,088,862 members\n",
    "\n",
    "I would like to find out if these two subreddits are distinct in their content (ie if they are separable/classifiable through modelling), and if so, I would choose to perform Sentiment Analysis on both Subreddits during Stock Selection.\n",
    "\n",
    "Conversely, if I discover that they are not distinct (ie not clearly separable/classifiable through modelling), then I would just pick one of the two Subreddits in my Sentiment Analysis.\n",
    "\n",
    "[Further Reading: Subreddit Descriptions](https://www.investing.com/academy/stocks/reddit-meme-stocks-to-buy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would create a few models to perform binary text classification on the two Subreddits. With the intention of attaining a good degree of separation between the two Subreddit classes. This is measured by the Receiver Operating Characteristics Area Under the Curve (ROC-AUC). ROC curve is a probability curve that shows model performance at all classification threshold with 2 parameters which is True Predicted Positive (TPR) and False Predicted Positive (FPR). The larger the area under the curve represents the larger the degree of separability between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will import the necessary charts in this section. For detailed scraping and EDA, please refer to the following Jupyter Notebooks\n",
    "\n",
    "    1. log-reg-model (Version 2).ipynb\n",
    "    2. random-forest-model (Version 2).ipynb\n",
    "    3. naive-bayes-model.ipynb\n",
    "    4. reddit-scrape.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All libraries used in this project are listed here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, recall_score, precision_score,accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Scraped Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please Refer to the Jupyter Notebook 'reddit-scrape.ipynb' for the full scraping code.\n",
    "\n",
    "I will describe briefly, the stages of cleaning in my EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "investing_df = pd.read_csv('datasets/investing.csv')\n",
    "stockmarket_df = pd.read_csv('datasets/stockmarket.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Hey guys, Iâ€™m a 22 year old male, I grew up wi...\n",
       "1                                            [removed]\n",
       "2                                            [removed]\n",
       "3                                            [removed]\n",
       "4    Hello Redditors ðŸ‘‹ \\n\\nI work as a Investment C...\n",
       "Name: selftext, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "investing_df['selftext'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                  NaN\n",
       "1    [Link to the full article (4 min read)](https:...\n",
       "2                                                  NaN\n",
       "3                                                  NaN\n",
       "4                                                  NaN\n",
       "Name: selftext, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stockmarket_df['selftext'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see from the above datasets, there were a lot of posts with [removed] and NaN. I removed these together with digits an non-letters.\n",
    "\n",
    "- Missing values were removed as there is no logical way to impute non numerical data.\n",
    "\n",
    "- I also removed stop words and hot encoded the target vector ('investing': 0, 'StockMarket': 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Stats|Logistic Regression (CVEC)|Logistic Regression (TVEC)|Random Forest (CVEC)|Random Forest (TVEC)|Naive Bayes (CVEC)|Naive Bayes (TVEC)|\n",
    "|----|----|----|----|----|----|----|\n",
    "|Recall/Sensitivity|0.7036|0.7222|0.76|0.368|0.534|0.681|\n",
    "|Precision|0.6402|0.718|0.778|0.629|0.834|0.773|\n",
    "|TP|952|2020|2168|1048|1320|1683|\n",
    "|TN|2980|2725|2521|2896|2376|2144|\n",
    "|FP|535|790|994|619|263|495|\n",
    "|FN|1899|831|683|1803|1153|790|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations from  Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see from the stats that both Logistic Regression (TVEC) and Random Forest (CVEC) models performed similarly. While Naive Bayes (TVEC) performed relatively well.\n",
    "\n",
    "- In our case, precision score is important as we want to accurately identify r/StockMarket posts. In this case, our  models have only slight differences in our most scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC AUC Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (Count Vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\logregcvecroc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (TFID Vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\logregtvecroc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (Count Vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\rfcvecroc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (TFID Vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\rftvecroc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (Count Vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\nbcvecroc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (TFID Vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images\\nbtvecroc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more area under a curve means better better separated our distributions our model give. When our ROC AUC is closer to 1, then our positive and negative populations are better separated which means the model is better. From this graph, we can see that Logistic Regression gives a much better curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model chosen is Naive Bayes with Count or TFID Vectorizer\n",
    "\n",
    "- From the model stats and ROC AUC curve, Naive Bayes Models (both CVEC and TVEC) and Logistic Regression with TFID Vectorizer performed the best. But NAive Bayes had a slightly higher ROCAUC score of 0.83\n",
    "- We can look at other models (KNN) to see if they can do better than our current models.\n",
    "- To further build on this project, we can look at sentiment analysis on the 2 topics. We can also look at specific topics in each subreddit that are unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1131d84b9e97d700f196cec3f143c1c5ca4787d89ba01101505d30befb8a4c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
